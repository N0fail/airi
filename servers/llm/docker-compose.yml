services:
  llama:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: llama-server
    ports:
      - "8000:8000"
    volumes:
      - ../../models/llm:/models
    command:
      [
        "--model", "/models/current",
        "--host", "0.0.0.0",
        "--port", "8000"
      ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    stdin_open: true
    tty: true
    restart: unless-stopped

